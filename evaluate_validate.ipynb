{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These results are validated manually.\n",
    "\n",
    "# Here is the results and evaluation metrics.\n",
    "# I also provide false negatives, related to explanations in the final paper.\n",
    "\n",
    "\n",
    "# Load the validated predictions Excel file\n",
    "df = pd.read_excel('validation_and_falsepred_datasets\\\\validated_resmi_gazete_data_2013.xlsx')\n",
    "\n",
    "# Calculate false positives and false negatives\n",
    "false_positives = df[(df['Predicted'] == 1) & (df['Target'] == 0)]\n",
    "false_negatives = df[(df['Predicted'] == 0) & (df['Target'] == 1)]\n",
    "\n",
    "# Calculate false positive and false negative ratios\n",
    "false_positive_ratio = len(false_positives) / len(df[df['Target'] == 0])\n",
    "false_negative_ratio = len(false_negatives) / len(df[df['Target'] == 1])\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(df['Target'], df['Predicted'])\n",
    "\n",
    "# Print results\n",
    "print(f\"False Positive Ratio: {false_positive_ratio:.4f}\")\n",
    "print(f\"False Negative Ratio: {false_negative_ratio:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Print out false positives and false negatives\n",
    "print(\"\\nFalse Positives:\")\n",
    "print(false_positives[['Date', 'Category', 'Subtitle', 'Predicted', 'Target']])\n",
    "\n",
    "print(\"\\nFalse Negatives:\")\n",
    "print(false_negatives[['Date', 'Category', 'Subtitle', 'Predicted', 'Target']])\n",
    "\n",
    "df_2013_false_pred = pd.concat([false_positives, false_negatives], ignore_index=True)\n",
    "df_2013_false_pred.to_excel('resmi_gazete_2013_false_pred.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Prediction notebook for training with 2012-2013 to predict 47 samples of 2006-2024\n",
    "# Using manually validated sets provided below for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample1 = pd.read_excel('validation_and_falsepred_datasets\\\\validated_sample1_prediction_2006_to_2024_not12_not13.xlsx', usecols=[\"Date\", \"Category\", \"Subtitle\", \"Predicted\", \"Target\"])\n",
    "df_sample2 = pd.read_excel('validation_and_falsepred_datasets\\\\validated_sample2_prediction_2006_to_2024_not12_not13.xlsx', usecols=[\"Date\", \"Category\", \"Subtitle\", \"Predicted\", \"Target\"])\n",
    "df_sample17 = pd.read_excel('validation_and_falsepred_datasets\\\\validated_sample17_prediction_2006_to_2024_not12_not13.xlsx', usecols=[\"Date\", \"Category\", \"Subtitle\", \"Predicted\", \"Target\"])\n",
    "df_sample41 = pd.read_excel('validation_and_falsepred_datasets\\\\validated_sample41_prediction_2006_to_2024_not12_not13.xlsx', usecols=[\"Date\", \"Category\", \"Subtitle\", \"Predicted\", \"Target\"])\n",
    "\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.concat([df_sample1, df_sample2, df_sample17, df_sample41], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new Excel file\n",
    "merged_df.to_excel('merged_validated_resmi_gazete_data_sample1_2_17_41.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Load the validated predictions Excel file\n",
    "df_sample_test = pd.read_excel('validation_and_falsepred_datasets\\\\merged_validated_resmi_gazete_data_sample1_2_17_41.xlsx')\n",
    "\n",
    "# Calculate false positives and false negatives\n",
    "false_positives = df_sample_test[(df_sample_test['Predicted'] == 1) & (df_sample_test['Target'] == 0)]\n",
    "false_negatives = df_sample_test[(df_sample_test['Predicted'] == 0) & (df_sample_test['Target'] == 1)]\n",
    "\n",
    "# Calculate false positive and false negative ratios\n",
    "false_positive_ratio = len(false_positives) / len(df_sample_test[df_sample_test['Target'] == 0])\n",
    "false_negative_ratio = len(false_negatives) / len(df_sample_test[df_sample_test['Target'] == 1])\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(df_sample_test['Target'], df_sample_test['Predicted'])\n",
    "\n",
    "# Print results\n",
    "print(f\"False Positive Ratio: {false_positive_ratio:.4f}\")\n",
    "print(f\"False Negative Ratio: {false_negative_ratio:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Print out false positives and false negatives\n",
    "print(\"\\nFalse Positives:\")\n",
    "print(false_positives[['Date', 'Category', 'Subtitle', 'Predicted', 'Target']])\n",
    "\n",
    "print(\"\\nFalse Negatives:\")\n",
    "print(false_negatives[['Date', 'Category', 'Subtitle', 'Predicted', 'Target']])\n",
    "\n",
    "df_samples_false_pred = pd.concat([false_positives, false_negatives], ignore_index=True)\n",
    "df_samples_false_pred.to_excel('falsepred_validated_resmi_gazete_data_sample1_2_17_41.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load spaCy Turkish model\n",
    "nlp = spacy.load('tr_core_news_md')\n",
    "\n",
    "# Combine default stop words with additional ones\n",
    "additional_stop_words = set([\n",
    "    'anonim', 'şirketi', 'acele', 'müdürlüğü', 'karar', 'taşınmazların', 'hakkında', \n",
    "    'kapsamında', 'kv', 'projesi', '-', '154', 'trafo', 'merkezi', '380', \"kamulaştırılması\", \n",
    "    'işleri', 'devlet', 'amacıyla', 'yapımı', 'kanun', 'değişiklik', \n",
    "    'ilişkin', 'yılı', 'yılında', 'programında', 'yapılmasına', 'hazine', \n",
    "    'kalkınmalarının', 'hazineye', 'yerlerin', 'adına', 'değerlendirilmesi', \n",
    "    'sınırları', 'dışına', 'çıkarılan', 'satışı', 'irtifak', 'şeklinde', \n",
    "    'yerlerinin', 'kurulmak', 'gabarisinin', 'suretiyle', 'hakkı', \n",
    "    'mülkiyet', 'direk', 'salınım', ',', \"kamulaştırılmasıhakkında\", \n",
    "    'verilmesi', 'izin', 'kurulmasına', 'şahinbey', 'dayalı', 'gaziantep', \n",
    "    'kurulacak', 'ardahan', 'çine', 'gördes', 'dinar', 'çeşme', \n",
    "    'yomra', 'şirvan', 'almus', 'siirt', 'suçatı', 'spor', 'salonunun', \n",
    "    'jimnastik', 'oyunları', 'tesisleri', 'atıcılık', 'akdeniz', 'stadyum', \n",
    "    'mersin’de', \"mersin\", 'planlanan', 'inşası', 'pervari', 'saruhanlı', 'seydioğlu', \n",
    "    'cumhuriyeti', 'hükümeti', 'bulunduğuna', 'uygun', 'onaylanmasının', \n",
    "    'arasında', 'alanında', 'anlaşmasının', \"ilişkin\",\n",
    "    'konulu', 'zaptının', 'kosova', 'uğrayabilecek', \"yapımıamacıyla\", \"bazıtaşınmazların\",\n",
    "    'yerlerde', 'muhdesatın', \"genel\",\"ïli\", \"ïlçesinde\",\n",
    "    'üzerindeki', 'duyulan', 'ihtiyaç', 'fatih', 'pervari', \n",
    "    'kökenli', 'artırılması', '%', 'oranında', \n",
    "    'pazarlama', 'için', 'kurulu', 'ziyaret', 'kağızman', 'kars', \n",
    "    'ilçesi', 'şanlıurfa', 'ili', 'merkez', 'elektrifikasyon', \n",
    "    'sinyalizasyon', 'hat', 'bucağı', 'ilan', 'edilen', 'yapımları', 'yeni', 'üniversitesi', 'ağrı', 'bağlı', \n",
    "    'istanbul', 'teşkilatı', \"geçen\",\n",
    "    'teşkilatında', 'çeçen', 'seferihisar', 'urla', 'kapsama', \n",
    "    'sigortaları', 'alınacak', 'riskler', 'ürünler',  'bölgeler', \n",
    "    'oranlarına', 'prim', 'havuzu', 'kanunu', 'iş', 'gebze', \n",
    "    'hattı', 'izmir', 'istanbul', \n",
    "    'uygulanması', 'yer', 'konak', 'karayolları', 'belediyesi', 'yürütülen', \n",
    "    'gelişim', 'içerisinde', 'manisa', 'dokap', \"ïşleri\",\n",
    "    'kılacak', 'ölçüm', 'lüzumlu', 'kurulmasını', \"ïşlerinde\", \n",
    "    'kanunda', 'çıkarılması', 'tespiti', \n",
    "    'kurulu', 'toplu', 'idaresi', 'başkanlığı', \n",
    "    'kişi', 'tüzel', 'enerji', 'tesis', 'kurumu', 'düzenleme', \n",
    "    'piyasası',  'santralinin', 'tescil', 'edilmek', 'hazine'\n",
    "])\n",
    "stop_words = tr_stop_words.union(additional_stop_words)\n",
    "\n",
    "# Normalize the stop words\n",
    "stop_words = {unicodedata.normalize('NFKD', word) for word in stop_words}\n",
    "\n",
    "# Convert stop words to list for CountVectorizer\n",
    "stop_words_list = list(stop_words)\n",
    "\n",
    "# Function to preprocess text using spaCy and normalize unicode\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'^\\d{4}/\\d{4}|\\d{4}', '', text).strip()\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.text for token in doc if token.text not in stop_words and not token.is_punct and not token.is_digit]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load the annotated data\n",
    "df = pd.read_excel('validation_and_falsepred_datasets\\\\merged_validated_resmi_gazete_data_2012_2013.xlsx')\n",
    "\n",
    "# Filter by Target value equals to 1\n",
    "df_filtered = df[df['Target'] == 1]\n",
    "\n",
    "# Apply preprocessing to the 'Subtitle' column\n",
    "df_filtered['ProcessedSubtitle'] = df_filtered['Subtitle'].apply(preprocess_text)\n",
    "\n",
    "# Create a count vectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stop_words_list)\n",
    "data_vectorized = vectorizer.fit_transform(df_filtered['ProcessedSubtitle'])\n",
    "\n",
    "# Create a Gensim dictionary and corpus\n",
    "texts = [text.split() for text in df_filtered['ProcessedSubtitle']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Function to compute coherence score without multiprocessing\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=4, step=1, random_state=42):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=random_state)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Compute coherence values for different numbers of topics\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=4, limit=35, step=1)\n",
    "\n",
    "# Plot coherence values\n",
    "limit = 35; start = 4; step = 1\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Function to run LDA and save results\n",
    "def run_lda(num_topics):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    df_filtered[f'LDA_Topic_{num_topics}'] = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
    "    return df_filtered[['Date', 'Category', 'Subtitle', 'Target', f'LDA_Topic_{num_topics}']], lda_model\n",
    "\n",
    "# Select the best model and run LDA with the best number of topics\n",
    "best_num_topics = x[coherence_values.index(max(coherence_values))]\n",
    "print(f\"Best number of topics: {best_num_topics}\")\n",
    "\n",
    "lda_result, lda_model = run_lda(best_num_topics)\n",
    "lda_result['Model_Name'] = f'LDA_Topic_{best_num_topics}'\n",
    "\n",
    "# Save LDA results to Excel file\n",
    "lda_result.to_excel(f'lda_topics_{best_num_topics}_withstops.xlsx', index=False)\n",
    "\n",
    "# Function to visualize the top words for each topic\n",
    "def visualize_topics(model_name, model, num_topics, vectorizer=None):\n",
    "    cols = 2\n",
    "    rows = math.ceil(num_topics / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 15), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_idx, ax in enumerate(axes):\n",
    "        if topic_idx >= num_topics:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        topic_words = dict(model.show_topic(topic_idx, 20))\n",
    "        if topic_words:  # Only generate word cloud if topic_words is not empty\n",
    "            wordcloud = WordCloud(font_path='DejaVuSans.ttf', background_color='white').generate_from_frequencies(topic_words)\n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.set_title(f\"Topic {topic_idx + 1}\")\n",
    "        else:\n",
    "            ax.set_title(f\"Topic {topic_idx + 1} - No Words\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Top words for each topic - {model_name}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Visualize LDA Topics for the best number of topics\n",
    "visualize_topics(f'LDA_Topic_{best_num_topics}', lda_model, best_num_topics)\n",
    "\n",
    "# Function to save most weighted words\n",
    "def save_most_weighted_words(model_name, model, num_topics):\n",
    "    most_weighted_words = []\n",
    "\n",
    "    for topic_idx in range(num_topics):\n",
    "        topic_words = dict(model.show_topic(topic_idx, 20))\n",
    "        for word, weight in topic_words.items():\n",
    "            most_weighted_words.append({\n",
    "                'Model_Name': model_name,\n",
    "                'Topic_No': topic_idx + 1,\n",
    "                'Word': word,\n",
    "                'Weight': weight\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(most_weighted_words)\n",
    "\n",
    "most_weighted_words_df = save_most_weighted_words(f'LDA_Topic_{best_num_topics}', lda_model, best_num_topics)\n",
    "\n",
    "# Save the most weighted words to an Excel file\n",
    "most_weighted_words_df.to_excel(f'most_weighted_words_LDAtopics{best_num_topics}_withstops.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load spaCy Turkish model\n",
    "nlp = spacy.load('tr_core_news_md')\n",
    "\n",
    "# Combine default stop words with additional ones\n",
    "additional_stop_words = set([\n",
    "    'anonim', 'şirketi', 'acele', 'müdürlüğü', 'karar', 'taşınmazların', 'hakkında', \"kamulaştırılması\",\n",
    "    'kapsamında', 'kv', 'projesi', '-', '154', 'trafo', 'merkezi', '380', \n",
    "    'işleri', 'devlet', 'amacıyla', 'yapımı', 'kanun', 'değişiklik', \n",
    "    'ilişkin', 'yılı', 'yılında', 'programında', 'yapılmasına', 'hazine', \n",
    "    'kalkınmalarının', 'hazineye', 'yerlerin', 'adına', 'değerlendirilmesi', \n",
    "    'sınırları', 'dışına', 'çıkarılan', 'satışı', 'irtifak', 'şeklinde', \n",
    "    'yerlerinin', 'kurulmak', 'gabarisinin', 'suretiyle', 'hakkı', \n",
    "    'mülkiyet', 'direk', 'salınım', ',', \n",
    "    'verilmesi', 'izin', 'kurulmasına', 'şahinbey', 'dayalı', 'gaziantep', \n",
    "    'kurulacak', 'ardahan', 'çine', 'gördes', 'dinar', 'çeşme', \n",
    "    'yomra', 'şirvan', 'almus', 'siirt', 'suçatı', 'spor', 'salonunun', \n",
    "    'jimnastik', 'oyunları', 'tesisleri', 'atıcılık', 'akdeniz', 'stadyum', \n",
    "    'mersin’de', \"mersin\", 'planlanan', 'inşası', 'pervari', 'saruhanlı', 'seydioğlu', \n",
    "    'cumhuriyeti', 'hükümeti', 'bulunduğuna', 'uygun', 'onaylanmasının', \n",
    "    'arasında', 'işbirliği', 'alanında', 'anlaşmasının', \n",
    "    'konulu', 'zaptının', 'kosova', 'uğrayabilecek', \n",
    "    'yerlerde', 'muhdesatın', \n",
    "    'üzerindeki', 'duyulan', 'ihtiyaç', 'fatih', 'pervari', \n",
    "    'kökenli', 'artırılması', '%', 'oranında', \n",
    "    'pazarlama', 'için', 'kurulu', 'ziyaret', 'kağızman', 'kars', \n",
    "    'ilçesi', 'şanlıurfa', 'ili', 'merkez', 'elektrifikasyon', \n",
    "    'sinyalizasyon', 'hat', 'bucağı', 'ilan', 'edilen', 'yapımları', 'yeni', 'üniversitesi', 'ağrı', 'taşra', 'bağlı', \n",
    "    'istanbul', 'teşkilatı', \n",
    "    'teşkilatında', 'çeçen', 'seferihisar', 'rüzgar', 'urla', 'kapsama', \n",
    "    'sigortaları', 'alınacak', 'riskler', 'ürünler', 'desteği', 'bölgeler', \n",
    "    'oranlarına', 'prim', 'havuzu', 'kanunu', 'iş', 'gebze', \n",
    "    'hattı', 'izmir', 'istanbul', 'jeotermal', \n",
    "    'uygulanması', 'yer', 'konak', 'karayolları', 'belediyesi', 'yürütülen', \n",
    "    'gelişim', 'içerisinde', 'manisa', 'dokap', \n",
    "    'kılacak', 'ölçüm', 'lüzumlu', 'kurulmasını', \n",
    "    'kanunda', 'çıkarılması', 'tespiti', \n",
    "    'kurulu', 'toplu', 'idaresi', 'başkanlığı', \n",
    "    'kişi', 'tüzel', 'enerji', 'tesis', 'kurumu', 'düzenleme', \n",
    "    'piyasası', 'hidroelektrik', 'santralinin', 'tescil', 'edilmek', 'hazine'\n",
    "])\n",
    "stop_words = tr_stop_words.union(additional_stop_words)\n",
    "\n",
    "# Normalize the stop words\n",
    "stop_words = {unicodedata.normalize('NFKD', word) for word in stop_words}\n",
    "\n",
    "# Convert stop words to list for CountVectorizer\n",
    "stop_words_list = list(stop_words)\n",
    "\n",
    "# Function to preprocess text using spaCy and normalize unicode\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'^\\d{4}/\\d{4}|\\d{4}', '', text).strip()\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.text for token in doc if token.text not in stop_words and not token.is_punct and not token.is_digit]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load the annotated data\n",
    "df = pd.read_excel('validation_and_falsepred_datasets\\\\merged_validated_resmi_gazete_data_2012_2013.xlsx')\n",
    "\n",
    "# Filter by Target value equals to 1\n",
    "df_filtered = df[df['Target'] == 1]\n",
    "# Load Turkish BERT model\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Embed the subtitles\n",
    "embeddings = sentence_model.encode(df_filtered['Subtitle'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Function to run BERTopic and save results\n",
    "def run_bertopic():\n",
    "    topic_model = BERTopic(nr_topics=\"auto\")\n",
    "    topics, probs = topic_model.fit_transform(df_filtered['Subtitle'].tolist(), embeddings)\n",
    "    df_filtered['BERTopic'] = topics\n",
    "    return df_filtered[['Date', 'Category', 'Subtitle', 'BERTopic']], topic_model\n",
    "\n",
    "bertopic_result, bertopic_model = run_bertopic()\n",
    "bertopic_result.to_excel('bertopic_topics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_most_weighted_words(model_name, model, num_topics, vectorizer=None):\n",
    "    most_weighted_words = []\n",
    "\n",
    "    for topic_idx in range(num_topics):\n",
    "        if model_name == 'BERTopic_Topic':\n",
    "            topic_info = model.get_topic(topic_idx)\n",
    "            if topic_info:\n",
    "                topic_words = dict(topic_info[:20])\n",
    "            else:\n",
    "                topic_words = {}\n",
    "\n",
    "        for word, weight in topic_words.items():\n",
    "            most_weighted_words.append({\n",
    "                'Model_Name': model_name,\n",
    "                'Topic_No': topic_idx + 1,\n",
    "                'Word': word,\n",
    "                'Weight': weight\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(most_weighted_words)\n",
    "\n",
    "most_weighted_words_df = pd.DataFrame()\n",
    "\n",
    "# Save the most weighted words for models with 14 topics\n",
    "most_weighted_words_df = pd.concat([\n",
    "\n",
    "    save_most_weighted_words('BERTopic_Topic', bertopic_model, 14)\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save the most weighted words to an Excel file\n",
    "most_weighted_words_df.to_excel('most_weighted_words_topics.xlsx', index=False)\n",
    "\n",
    "# Visualization of Top Words for Each Topic for Model with 14 Topics\n",
    "def visualize_topics(model_name, model, num_topics, vectorizer=None):\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(20, 15), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_idx, ax in enumerate(axes):\n",
    "        if topic_idx >= num_topics:\n",
    "            break\n",
    "        elif model_name == 'BERTopic_Topic':\n",
    "            topic_info = model.get_topic(topic_idx)\n",
    "            if topic_info:\n",
    "                topic_words = dict(topic_info[:20])\n",
    "            else:\n",
    "                topic_words = {}\n",
    "\n",
    "        if topic_words:  # Only generate word cloud if topic_words is not empty\n",
    "            wordcloud = WordCloud(background_color='white').generate_from_frequencies(topic_words)\n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.set_title(f\"Topic {topic_idx + 1}\")\n",
    "        else:\n",
    "            ax.set_title(f\"Topic {topic_idx + 1} - No Words\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Top words for each topic - {model_name}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Visualize BERTopic Topics\n",
    "visualize_topics('BERTopic_Topic', bertopic_model, 14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
