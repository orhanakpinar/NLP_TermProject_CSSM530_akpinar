{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold Train-Test on 2012\n",
    "# Full training on 2012 for predicting 2013\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('scraped_and_filtered_datasets\\\\filtered_resmi_gazete_data_2012.xlsx')\n",
    "df_2013 = pd.read_excel('scraped_and_filtered_datasets\\\\filtered_resmi_gazete_data_2013.xlsx')\n",
    "\n",
    "# Ensure the 'Target' column is of type integer\n",
    "df['Target'] = df['Target'].astype(int)\n",
    "\n",
    "# Define a Dataset class for PyTorch\n",
    "class SubtitleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) if self.labels is not None else len(self.encodings['input_ids'])\n",
    "\n",
    "# Use a Turkish BERT model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-uncased')\n",
    "\n",
    "# Tokenize the input text\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenize_function(df['Subtitle'].tolist())\n",
    "train_labels = df['Target'].tolist()\n",
    "\n",
    "# Create the PyTorch dataset\n",
    "train_dataset = SubtitleDataset(train_encodings, train_labels)\n",
    "\n",
    "# Ensure the output directory is a valid directory\n",
    "output_dir = './results'\n",
    "if os.path.exists(output_dir) and os.path.isfile(output_dir):\n",
    "    os.remove(output_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Define the model and training arguments\n",
    "model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-turkish-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # Use the directory path here\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(df):\n",
    "    train_texts, val_texts = df['Subtitle'].iloc[train_index].tolist(), df['Subtitle'].iloc[val_index].tolist()\n",
    "    train_labels, val_labels = df['Target'].iloc[train_index].tolist(), df['Target'].iloc[val_index].tolist()\n",
    "\n",
    "    train_encodings = tokenize_function(train_texts)\n",
    "    val_encodings = tokenize_function(val_texts)\n",
    "\n",
    "    train_dataset = SubtitleDataset(train_encodings, train_labels)\n",
    "    val_dataset = SubtitleDataset(val_encodings, val_labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda p: {\n",
    "            'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1)),\n",
    "            'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='weighted')\n",
    "        }\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    accuracy_scores.append(metrics['eval_accuracy'])\n",
    "    f1_scores.append(metrics['eval_f1'])\n",
    "\n",
    "print(\"10-Fold Cross-Validation Results:\")\n",
    "print(\"Average Accuracy: \", sum(accuracy_scores) / len(accuracy_scores))\n",
    "print(\"Average F1 Score: \", sum(f1_scores) / len(f1_scores))\n",
    "\n",
    "# Train on the full 2012 dataset\n",
    "train_encodings = tokenize_function(df['Subtitle'].tolist())\n",
    "train_labels = df['Target'].tolist()\n",
    "\n",
    "train_dataset = SubtitleDataset(train_encodings, train_labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Predict on the 2013 data\n",
    "test_encodings = tokenize_function(df_2013['Subtitle'].tolist())\n",
    "test_dataset = SubtitleDataset(test_encodings)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "df_2013['Predicted'] = predicted_labels\n",
    "\n",
    "# Save the predictions\n",
    "df_2013.to_excel('predicted_resmi_gazete_data_2013.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge validated datasets\n",
    "\n",
    "# Load the 2012 and 2013 validated data\n",
    "df_2012 = pd.read_excel('validation_and_falsepred_datasets\\\\validated_resmi_gazete_data_2012.xlsx', usecols=[\"Date\", \"Category\", \"Subtitle\", \"Target\"])\n",
    "df_2013 = pd.read_excel('validation_and_falsepred_datasets\\\\validated_resmi_gazete_data_2013.xlsx', usecols=[\"Date\", \"Category\", \"Subtitle\", \"Target\"])\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.concat([df_2012, df_2013], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new Excel file\n",
    "merged_df.to_excel('merged_validated_resmi_gazete_data_2012_2013.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train BERT model with 2012 and 2013 annotated dataset.\n",
    "# Predict filtered data from 2006 to 2024, excluded 2012 and 2013\n",
    "# Use 47 samples for testing 5029 entries of from 2006 data.\n",
    "# Store samples, then validate 4 of them for evaluation.\n",
    "# See Evaluation notebook for metrics.\n",
    "\n",
    "\n",
    "# Load the annotated data\n",
    "annotated_df = pd.read_excel('validation_and_falsepred_datasets\\\\merged_validated_resmi_gazete_data_2012_2013.xlsx')\n",
    "\n",
    "# Load the unannotated data\n",
    "unannotated_df = pd.read_excel('scraped_and_filtered_datasets\\\\filtered_resmi_gazete_data_from2006_not12_not13.xlsx')\n",
    "\n",
    "# Ensure reproducibility\n",
    "unannotated_df = unannotated_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into 47 random samples\n",
    "samples = np.array_split(unannotated_df, 47)\n",
    "\n",
    "# Ensure the 'Target' column is of type integer\n",
    "annotated_df['Target'] = annotated_df['Target'].astype(int)\n",
    "\n",
    "\n",
    "# Prepare the data for training\n",
    "\n",
    "# Define a Dataset class for PyTorch\n",
    "class SubtitleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) if self.labels is not None else len(self.encodings['input_ids'])\n",
    "\n",
    "# Use a Turkish BERT model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-uncased')\n",
    "\n",
    "# Tokenize the input text\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenize_function(annotated_df['Subtitle'].tolist())\n",
    "train_labels = annotated_df['Target'].tolist()\n",
    "\n",
    "# Create the PyTorch dataset\n",
    "train_dataset = SubtitleDataset(train_encodings, train_labels)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Define the model and training arguments\n",
    "model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-turkish-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1)),\n",
    "        'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='weighted')\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "#Predict and save the results\n",
    "\n",
    "# Function to predict and save each sample\n",
    "def predict_and_save(sample, sample_num):\n",
    "    # Ensure all subtitles are strings\n",
    "    sample['Subtitle'] = sample['Subtitle'].astype(str)\n",
    "\n",
    "    sample_encodings = tokenize_function(sample['Subtitle'].tolist())\n",
    "    sample_dataset = SubtitleDataset(sample_encodings)\n",
    "\n",
    "    predictions = trainer.predict(sample_dataset)\n",
    "    predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "    sample['Predicted'] = predicted_labels\n",
    "\n",
    "    # Save the predictions to a new Excel file\n",
    "    sample.to_excel(f'sample{sample_num}_prediction_2006_to_2024_not12_not13.xlsx', index=False)\n",
    "\n",
    "# Predict and save each sample\n",
    "for i, sample in enumerate(samples):\n",
    "    predict_and_save(sample, i + 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
