{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine URL loop and beautifulsoup parser\n",
    "## Year 2012, Filtered\n",
    "## Apply\n",
    "\n",
    "base_url = 'https://www.resmigazete.gov.tr/eskiler'\n",
    "years = [2012]  # Use a specific year for demonstration\n",
    "months = [\"{:02d}\".format(i) for i in range(1, 13)]  # Specific month for demonstration\n",
    "days = [\"{:02d}\".format(i) for i in range(1, 32)]  # Specific days for demonstration\n",
    "\n",
    "# Data storage\n",
    "data_rg = []\n",
    "\n",
    "# Iterate through each date\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        for day in days:\n",
    "            url = f\"{base_url}/{year}/{month}/{year}{month}{day}.htm\"\n",
    "            print(\"Processing URL:\", url)\n",
    "            time.sleep(1)  # Delay to prevent too frequent requests\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                elements = soup.select('#AutoNumber1 p')\n",
    "                \n",
    "                current_category = None\n",
    "                has_subtitles = False\n",
    "                \n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    text = re.sub(r'\\r\\n|\\r|\\n|\\xa0', ' ', text)  # Clean the text\n",
    "\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    if text.isupper():\n",
    "                        if current_category and not has_subtitles:\n",
    "                            data_rg.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': 'NA'})\n",
    "                        current_category = text\n",
    "                        has_subtitles = False\n",
    "                    elif re.match(r'^[–—-][–—-]\\s', text) or re.match(r'^[a-z]\\s-\\s', text) or re.match(r'^–\\s', text) or re.match(r'^\\d+', text):\n",
    "                        if current_category:\n",
    "                            data_rg.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': text.strip()})\n",
    "                            has_subtitles = True\n",
    "\n",
    "                if current_category and not has_subtitles:\n",
    "                    data_rg.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': 'NA'})\n",
    "            else:\n",
    "                print(\"Failed to retrieve the webpage for URL:\", url)\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df_rg = pd.DataFrame(data_rg)\n",
    "\n",
    "## Filter for necessary categories\n",
    "## Annotated 576 rows\n",
    "\n",
    "## Apply\n",
    "df_rg.to_excel('scraped_data_2012.xlsx', index=False)\n",
    "\n",
    "categories_to_include = [\"KANUN\", \"KANUNLAR\", \"BAKANLAR KURULU KARARLARI\", \"BAKANLAR KURULU KARARI\"]\n",
    "\n",
    "# Filter the DataFrame to only include rows with the specified categories\n",
    "filtered_df = df_rg[df_rg['Category'].isin(categories_to_include)]\n",
    "\n",
    "# Optionally, save this filtered DataFrame to a new CSV file\n",
    "filtered_df.to_excel('filtered_resmi_gazete_data_2012.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2013, Filtered, Saved, Full Code Block\n",
    "\n",
    "\n",
    "base_url = 'https://www.resmigazete.gov.tr/eskiler'\n",
    "years = [2013]  # Use a specific year for demonstration\n",
    "months = [\"{:02d}\".format(i) for i in range(1, 13)]  # Specific month for demonstration\n",
    "days = [\"{:02d}\".format(i) for i in range(1, 32)]  # Specific days for demonstration\n",
    "\n",
    "# Data storage\n",
    "data_rg_2013 = []\n",
    "\n",
    "# Iterate through each date\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        for day in days:\n",
    "            url = f\"{base_url}/{year}/{month}/{year}{month}{day}.htm\"\n",
    "            print(\"Processing URL:\", url)\n",
    "            time.sleep(3)  # Delay to prevent too frequent requests\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                elements = soup.select('#AutoNumber1 p')\n",
    "                \n",
    "                current_category = None\n",
    "                has_subtitles = False\n",
    "                \n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    text = re.sub(r'\\r\\n|\\r|\\n|\\xa0', ' ', text)  # Clean the text\n",
    "\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    if text.isupper():\n",
    "                        if current_category and not has_subtitles:\n",
    "                            data_rg_2013.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': 'NA'})\n",
    "                        current_category = text\n",
    "                        has_subtitles = False\n",
    "                    elif re.match(r'^[–—-][–—-]\\s', text) or re.match(r'^[a-z]\\s-\\s', text) or re.match(r'^–\\s', text) or re.match(r'^\\d+', text):\n",
    "                        if current_category:\n",
    "                            data_rg_2013.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': text.strip()})\n",
    "                            has_subtitles = True\n",
    "\n",
    "                if current_category and not has_subtitles:\n",
    "                    data_rg_2013.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': 'NA'})\n",
    "            else:\n",
    "                print(\"Failed to retrieve the webpage for URL:\", url)\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "data_rg_2013 = pd.DataFrame(data_rg_2013)\n",
    "\n",
    "data_rg_2013.to_excel('scraped_data_2013.xlsx', index=False)\n",
    "\n",
    "categories_to_include = [\"KANUN\", \"KANUNLAR\", \"BAKANLAR KURULU KARARLARI\", \"BAKANLAR KURULU KARARI\"]\n",
    "\n",
    "# Filter the DataFrame to only include rows with the specified categories\n",
    "filtered_df_2013 = data_rg_2013[data_rg_2013['Category'].isin(categories_to_include)]\n",
    "\n",
    "# Optionally, save this filtered DataFrame to a new CSV file\n",
    "filtered_df_2013.to_excel('filtered_resmi_gazete_data_2013.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2013 worked well, now finally, I scrape from 2007 to 2024\n",
    "# Not 2012, not 2013, as they will be my training dataset.\n",
    "\n",
    "\n",
    "\n",
    "base_url = 'https://www.resmigazete.gov.tr/eskiler'\n",
    "years = [2006, 2007, 2008, 2009, 2010, 2011, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]  # Use a specific year for demonstration\n",
    "months = [\"{:02d}\".format(i) for i in range(1, 13)]  # Specific month for demonstration\n",
    "days = [\"{:02d}\".format(i) for i in range(1, 32)]  # Specific days for demonstration\n",
    "\n",
    "# Data storage\n",
    "data_rg_from2006_not12_not13 = []\n",
    "\n",
    "# Iterate through each date\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        for day in days:\n",
    "            url = f\"{base_url}/{year}/{month}/{year}{month}{day}.htm\"\n",
    "            print(\"Processing URL:\", url)\n",
    "            time.sleep(5)  # Delay to prevent too frequent requests\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                elements = soup.select('#AutoNumber1 p')\n",
    "                \n",
    "                current_category = None\n",
    "                has_subtitles = False\n",
    "                \n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    text = re.sub(r'\\r\\n|\\r|\\n|\\xa0', ' ', text)  # Clean the text\n",
    "\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    if text.isupper():\n",
    "                        if current_category and not has_subtitles:\n",
    "                            data_rg_from2006_not12_not13.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': 'NA'})\n",
    "                        current_category = text\n",
    "                        has_subtitles = False\n",
    "                    elif re.match(r'^[–—-][–—-]\\s', text) or re.match(r'^[a-z]\\s-\\s', text) or re.match(r'^–\\s', text) or re.match(r'^\\d+', text):\n",
    "                        if current_category:\n",
    "                            data_rg_from2006_not12_not13.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': text.strip()})\n",
    "                            has_subtitles = True\n",
    "\n",
    "                if current_category and not has_subtitles:\n",
    "                    data_rg_from2006_not12_not13.append({'Date': f\"{year}-{month}-{day}\", 'Category': current_category, 'Subtitle': 'NA'})\n",
    "            else:\n",
    "                print(\"Failed to retrieve the webpage for URL:\", url)\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "data_rg_from2006_not12_not13 = pd.DataFrame(data_rg_from2006_not12_not13)\n",
    "\n",
    "data_rg_from2006_not12_not13.to_excel('scraped_data_from2006_not12_not13.xlsx', index=False)\n",
    "\n",
    "categories_to_include = [\"KANUN\", \"KANUNLAR\", \"BAKANLAR KURULU KARARLARI\", \"BAKANLAR KURULU KARARI\", \"CUMHURBAŞKANI  KARARLARI\", \"CUMHURBAŞKANI  KARARI\"]\n",
    "\n",
    "# Filter the DataFrame to only include rows with the specified categories\n",
    "data_rg_from2006_not12_not13 = data_rg_from2006_not12_not13[data_rg_from2006_not12_not13['Category'].isin(categories_to_include)]\n",
    "\n",
    "# Optionally, save this filtered DataFrame to a new CSV file\n",
    "data_rg_from2006_not12_not13.to_excel('filtered_resmi_gazete_data_from2006_not12_not13.xlsx', index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
